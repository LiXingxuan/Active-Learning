{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\xuan\\envs\\ml\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#预测“未标注”样本集属于特定类别的概率\n",
    "def proba_func(x_init,y_init,x_choice,y_choice):\n",
    "    \n",
    "    #打乱“已标注”样本集\n",
    "    choice_list = [i for i in range(len(x_init))]\n",
    "    random.shuffle(choice_list)\n",
    "    \n",
    "    #将“已标注”样本集分为三份\n",
    "    num1 = int(len(x_init)/3)\n",
    "    num2 = int(len(x_init)*2/3)\n",
    "    \n",
    "    x_init1 = pd.concat([x_init.iloc[choice_list[:num1]]])\n",
    "    x_init2 = pd.concat([x_init.iloc[choice_list[num1:num2]]])\n",
    "    x_init3 = pd.concat([x_init.iloc[choice_list[num2:]]])\n",
    "    \n",
    "    y_init1 = pd.concat([y_init.iloc[choice_list[:num1]]])\n",
    "    y_init2 = pd.concat([y_init.iloc[choice_list[num1:num2]]])\n",
    "    y_init3 = pd.concat([y_init.iloc[choice_list[num2:]]])\n",
    "    \n",
    "    #创建三个分类器\n",
    "    gb_clf1 = GradientBoostingClassifier()\n",
    "    gb_clf2 = GradientBoostingClassifier()\n",
    "    gb_clf3 = GradientBoostingClassifier()\n",
    "\n",
    "    #训练分类器\n",
    "    gb_clf1.fit(x_init1,y_init1)  \n",
    "    gb_clf2.fit(x_init2,y_init2)\n",
    "    gb_clf3.fit(x_init3,y_init3)\n",
    "    \n",
    "    #预测“未标注”样本集属于特定类别的概率\n",
    "    proba1 = gb_clf1.predict_proba(x_choice)\n",
    "    proba2 = gb_clf2.predict_proba(x_choice)\n",
    "    proba3 = gb_clf3.predict_proba(x_choice)\n",
    "    \n",
    "    return proba1,proba2,proba3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#评估函数\n",
    "def scores_func(proba):\n",
    "    scores_sort = []\n",
    "    #proba形如[[0.1,0.9],[0.4,0.6],[0.7,0.3]]\n",
    "    for sc in proba:\n",
    "        col = 0\n",
    "        for p in sc:\n",
    "            #避免出现log0\n",
    "            if p in [0,1]:\n",
    "                col += 0\n",
    "            else:\n",
    "                col += -p*math.log(p,math.e)\n",
    "        scores_sort.append(col)\n",
    "    return scores_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#返回新的样本集：分别为“已标注”特征矩阵、类标；“未标注”特征矩阵、类标\n",
    "def new_dataset(x_init,y_init,x_choice,y_choice,scores_sort,each_size=-30):\n",
    "    #scores_sorted为scores_sort由小到大的索引\n",
    "    scores_sort = np.array(scores_sort)\n",
    "    scores_sorted = np.argsort(scores_sort)\n",
    "    \n",
    "    #“scores_sorted[each_size:]”即为 选取最大的 -each_size 个scores_sort\n",
    "    x_init = pd.concat([x_init,x_choice.iloc[scores_sorted[each_size:]]])\n",
    "    y_init = pd.concat([y_init,y_choice.iloc[scores_sorted[each_size:]]])\n",
    "    x_choice = pd.concat([x_choice.iloc[scores_sorted[:each_size]]])\n",
    "    y_choice = pd.concat([y_choice.iloc[scores_sorted[:each_size]]])\n",
    "    \n",
    "    return x_init,y_init,x_choice,y_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#基本qbc\n",
    "def choice_tra(x_init,y_init,x_choice,y_choice):\n",
    "\n",
    "    #（学习引擎）预测“未标注”样本集属于特定类别的概率\n",
    "    proba1,proba2,proba3 = proba_func(x_init,y_init,x_choice,y_choice)\n",
    "    \n",
    "    #（选择引擎）三个分类器分别计算全体“未标注”样本集的分数\n",
    "    scores1_sort = scores_func(proba1)  \n",
    "    scores2_sort = scores_func(proba2)\n",
    "    scores3_sort = scores_func(proba3)\n",
    "    \n",
    "    #（选择引擎）关键：投票熵和类条件后验最大熵相结合，加入scores_sort\n",
    "    scores_sort = []\n",
    "    for i in range(len(scores1_sort)):\n",
    "        col = max(scores1_sort[i],scores2_sort[i],scores3_sort[i])\n",
    "        scores_sort.append(col)\n",
    "\n",
    "    #根据scores_sort，选择“未标注”样本，交由专家标注，后加入“已标注”样本集，并从“未标注”样本集中剔除\n",
    "    x_init,y_init,x_choice,y_choice = new_dataset(x_init,y_init,x_choice,y_choice,scores_sort)\n",
    "    \n",
    "    return x_init,y_init,x_choice,y_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#委员会加权qbc\n",
    "def choice_committee_weighting(x_init,y_init,x_choice,y_choice):\n",
    "\n",
    "    choice_list = [i for i in range(len(x_init))]\n",
    "    random.shuffle(choice_list)\n",
    "    \n",
    "    num1 = int(len(x_init)/3)\n",
    "    num2 = int(len(x_init)*2/3)\n",
    "    \n",
    "    x_init1 = pd.concat([x_init.iloc[choice_list[:num1]]])\n",
    "    x_init2 = pd.concat([x_init.iloc[choice_list[num1:num2]]])\n",
    "    x_init3 = pd.concat([x_init.iloc[choice_list[num2:]]])\n",
    "    \n",
    "    y_init1 = pd.concat([y_init.iloc[choice_list[:num1]]])\n",
    "    y_init2 = pd.concat([y_init.iloc[choice_list[num1:num2]]])\n",
    "    y_init3 = pd.concat([y_init.iloc[choice_list[num2:]]])\n",
    "    \n",
    "    gb_clf1 = GradientBoostingClassifier()\n",
    "    gb_clf2 = GradientBoostingClassifier()\n",
    "    gb_clf3 = GradientBoostingClassifier()\n",
    "\n",
    "    gb_clf1.fit(x_init1,y_init1)\n",
    "    gb_clf2.fit(x_init2,y_init2)\n",
    "    gb_clf3.fit(x_init3,y_init3)\n",
    "    \n",
    "    #根据“已标注”样本集，计算三个分类器的精确度\n",
    "    score_weight1 = gb_clf1.score(x_init,y_init)\n",
    "    score_weight2 = gb_clf2.score(x_init,y_init)\n",
    "    score_weight3 = gb_clf3.score(x_init,y_init)\n",
    "    \n",
    "    proba1 = gb_clf1.predict_proba(x_choice)\n",
    "    proba2 = gb_clf2.predict_proba(x_choice)\n",
    "    proba3 = gb_clf3.predict_proba(x_choice)\n",
    "    \n",
    "    scores1_sort = scores_func(proba1)  \n",
    "    scores2_sort = scores_func(proba2)\n",
    "    scores3_sort = scores_func(proba3)\n",
    "    \n",
    "    #关键：每个scores_sort需乘上各自分类器的精度，即为委员会加权\n",
    "    scores_sort = []\n",
    "    for i in range(len(scores1_sort)):\n",
    "        col = max(scores1_sort[i]*score_weight1,scores2_sort[i]*score_weight2,scores3_sort[i]*score_weight3)\n",
    "        scores_sort.append(col)\n",
    "\n",
    "    x_init,y_init,x_choice,y_choice = new_dataset(x_init,y_init,x_choice,y_choice,scores_sort)\n",
    "    \n",
    "    return x_init,y_init,x_choice,y_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#多样性qbc\n",
    "def choice_diversity(x_init,y_init,x_choice,y_choice):\n",
    "\n",
    "    proba1,proba2,proba3 = proba_func(x_init,y_init,x_choice,y_choice)\n",
    "    \n",
    "    scores1_sort = scores_func(proba1)  \n",
    "    scores2_sort = scores_func(proba2)\n",
    "    scores3_sort = scores_func(proba3)\n",
    "    \n",
    "    #合并“已标注”和“未标注”特征矩阵\n",
    "    x_all = pd.concat([x_init,x_choice])\n",
    "    \n",
    "    #计算“未标注”样本集在全体样本集中距离最近的样本的索引\n",
    "    neigh = NearestNeighbors()\n",
    "    neigh.fit(x_all)\n",
    "    #只需取最近的一个其他样本，因为训练集用x_all，所以返回值最近是x_choice本身，因此需要返回2个索引值\n",
    "    distance_number = neigh.kneighbors([x_choice.iloc[i] for i in range(len(x_choice))] ,2, return_distance=False)\n",
    "    \n",
    "    \n",
    "    scores_sort = []\n",
    "    for i in range(len(scores1_sort)):\n",
    "        #多样性分数，计算与“未标记”样本最近的一个样本的相似度\n",
    "        diversity = pairwise_distances([x_choice.iloc[i]],x_all.iloc[distance_number[i][1:2]],metric=\"cosine\").sum()\n",
    "        col = max(scores1_sort[i],scores2_sort[i],scores3_sort[i])*diversity\n",
    "        scores_sort.append(col)\n",
    "            \n",
    "    x_init,y_init,x_choice,y_choice = new_dataset(x_init,y_init,x_choice,y_choice,scores_sort)\n",
    "    \n",
    "    return x_init,y_init,x_choice,y_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#密度qbc\n",
    "def choice_density(x_init,y_init,x_choice,y_choice,density_scope=11):\n",
    "\n",
    "    proba1,proba2,proba3 = proba_func(x_init,y_init,x_choice,y_choice)\n",
    "    \n",
    "    scores1_sort = scores_func(proba1)  \n",
    "    scores2_sort = scores_func(proba2)\n",
    "    scores3_sort = scores_func(proba3)\n",
    "    \n",
    "    #合并“已标注”和“未标注”特征矩阵\n",
    "    x_all = pd.concat([x_init,x_choice])\n",
    "    \n",
    "    #计算“未标注”样本集在全体样本集中距离最近的density_scope个样本的索引\n",
    "    neigh = NearestNeighbors()\n",
    "    neigh.fit(x_all)\n",
    "    distance_number = neigh.kneighbors([x_choice.iloc[i] for i in range(len(x_choice))], density_scope, return_distance=False)\n",
    "    \n",
    "    scores_sort = []\n",
    "    for i in range(len(scores1_sort)):\n",
    "        #密度分数，计算“未标注”样本最近的density_scope个样本的平均密度\n",
    "        density = ((density_scope-1)-pairwise_distances([x_choice.iloc[i]],x_all.iloc[distance_number[i][1:]],metric=\"cosine\").sum())/density_scope\n",
    "        col = max(scores1_sort[i],scores2_sort[i],scores3_sort[i])*density\n",
    "        scores_sort.append(col)\n",
    "            \n",
    "    x_init,y_init,x_choice,y_choice = new_dataset(x_init,y_init,x_choice,y_choice,scores_sort)\n",
    "    \n",
    "    return x_init,y_init,x_choice,y_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#平衡qbc\n",
    "def choice_balance(x_init,y_init,x_choice,y_choice,bal_scale=0.35,bigger_par=0.6):\n",
    "\n",
    "    proba1,proba2,proba3 = proba_func(x_init,y_init,x_choice,y_choice)\n",
    "    \n",
    "    scores1_sort = scores_func(proba1)  \n",
    "    scores2_sort = scores_func(proba2)\n",
    "    scores3_sort = scores_func(proba3)\n",
    "    \n",
    "    #计算数据集的平衡性，使用小样本集除大样本集\n",
    "    distribution_normal = y_init.value_counts()[1]\n",
    "    distribution_suspicious = y_init.value_counts()[2]\n",
    "    distribution = distribution_suspicious/(distribution_normal+distribution_suspicious)\n",
    "    \n",
    "    scores_sort = []\n",
    "    #若当前数据集的平衡性小于预期平衡，则分类器预测“未标注”样本为大样本集的样本需乘bal_scale，减小其分数\n",
    "    if distribution >= bal_scale:\n",
    "        for i in range(len(scores1_sort)):\n",
    "            col = max(scores1_sort[i],scores2_sort[i],scores3_sort[i])\n",
    "            scores_sort.append(col)\n",
    "    else:\n",
    "        for i in range(len(scores1_sort)):\n",
    "            if (proba1[i][0]+proba2[i][0]+proba3[i][0])/3 < 0.5:\n",
    "                col = max(scores1_sort[i],scores2_sort[i],scores3_sort[i])\n",
    "            else:\n",
    "                col = max(scores1_sort[i],scores2_sort[i],scores3_sort[i])*bigger_par\n",
    "            scores_sort.append(col)\n",
    "\n",
    "    x_init,y_init,x_choice,y_choice = new_dataset(x_init,y_init,x_choice,y_choice,scores_sort)\n",
    "    \n",
    "    return x_init,y_init,x_choice,y_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#密度+平衡 qbc，仅在选择分数那里混合两种因子\n",
    "def choice_density_balance(x_init,y_init,x_choice,y_choice,bal_scale=0.35,bigger_par=0.6,density_scope=11):\n",
    "\n",
    "    proba1,proba2,proba3 = proba_func(x_init,y_init,x_choice,y_choice)\n",
    "    \n",
    "    scores1_sort = scores_func(proba1)  \n",
    "    scores2_sort = scores_func(proba2)\n",
    "    scores3_sort = scores_func(proba3)\n",
    "    \n",
    "    distribution_normal = y_init.value_counts()[1]\n",
    "    distribution_suspicious = y_init.value_counts()[2]\n",
    "    distribution = distribution_suspicious/(distribution_normal+distribution_suspicious)\n",
    "    \n",
    "    neigh = NearestNeighbors()\n",
    "    neigh.fit(x_all)\n",
    "    distance_number = neigh.kneighbors([x_choice.iloc[i] for i in range(len(x_choice))], density_scope, return_distance=False)\n",
    "    \n",
    "    scores_sort = []\n",
    "    if distribution >= bal_scale:\n",
    "        for i in range(len(scores1_sort)):\n",
    "            density = (10-pairwise_distances([x_choice.iloc[i]],x_all.iloc[distance_number[i][1:]],metric=\"cosine\").sum())/10\n",
    "            col = max(scores1_sort[i],scores2_sort[i],scores3_sort[i])*density\n",
    "            scores_sort.append(col)\n",
    "    else:\n",
    "        for i in range(len(scores1_sort)):\n",
    "            density = (10-pairwise_distances([x_choice.iloc[i]],x_all.iloc[distance_number[i][1:]],metric=\"cosine\").sum())/10\n",
    "            if (proba1[i][0]+proba2[i][0]+proba3[i][0])/3 < 0.5:\n",
    "                col = max(scores1_sort[i],scores2_sort[i],scores3_sort[i])*density\n",
    "            else:\n",
    "                col = max(scores1_sort[i],scores2_sort[i],scores3_sort[i])*density*bigger_par\n",
    "            scores_sort.append(col)\n",
    "\n",
    "    x_init,y_init,x_choice,y_choice = new_dataset(x_init,y_init,x_choice,y_choice,scores_sort)\n",
    "    \n",
    "    return x_init,y_init,x_choice,y_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#多样性+密度\n",
    "def choice_diversity_density(x_init,y_init,x_choice,y_choice,density_scope=11):\n",
    "\n",
    "    proba1,proba2,proba3 = proba_func(x_init,y_init,x_choice,y_choice)\n",
    "    \n",
    "    scores1_sort = scores_func(proba1)  \n",
    "    scores2_sort = scores_func(proba2)\n",
    "    scores3_sort = scores_func(proba3)\n",
    "    \n",
    "    x_all = pd.concat([x_init,x_choice])\n",
    "    \n",
    "    neigh = NearestNeighbors()\n",
    "    neigh.fit(x_all)\n",
    "    distance_number = neigh.kneighbors([x_choice.iloc[i] for i in range(len(x_choice))] ,density_scope, return_distance=False)\n",
    "    \n",
    "    scores_sort = []\n",
    "    for i in range(len(scores1_sort)):\n",
    "        diversity = pairwise_distances([x_choice.iloc[i]],x_all.iloc[distance_number[i][1:2]],metric=\"cosine\").sum()\n",
    "        density = (10-pairwise_distances([x_choice.iloc[i]],x_all.iloc[distance_number[i][1:]],metric=\"cosine\").sum())/10\n",
    "        col = max(scores1_sort[i],scores2_sort[i],scores3_sort[i])*diversity*density\n",
    "        scores_sort.append(col)\n",
    "            \n",
    "    x_init,y_init,x_choice,y_choice = new_dataset(x_init,y_init,x_choice,y_choice,scores_sort)\n",
    "    \n",
    "    return x_init,y_init,x_choice,y_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#密度+平衡+委员会加权 qbc，同上\n",
    "def choice_density_balance_committee_weighting(x_init,y_init,x_choice,y_choice,bal_scale=0.35,bigger_par=0.6,density_scope=11):\n",
    "\n",
    "    choice_list = [i for i in range(len(x_init))]\n",
    "    random.shuffle(choice_list)\n",
    "    \n",
    "    num1 = int(len(x_init)/3)\n",
    "    num2 = int(len(x_init)*2/3)\n",
    "    \n",
    "    x_init1 = pd.concat([x_init.iloc[choice_list[:num1]]])\n",
    "    x_init2 = pd.concat([x_init.iloc[choice_list[num1:num2]]])\n",
    "    x_init3 = pd.concat([x_init.iloc[choice_list[num2:]]])\n",
    "    \n",
    "    y_init1 = pd.concat([y_init.iloc[choice_list[:num1]]])\n",
    "    y_init2 = pd.concat([y_init.iloc[choice_list[num1:num2]]])\n",
    "    y_init3 = pd.concat([y_init.iloc[choice_list[num2:]]])\n",
    "    \n",
    "    gb_clf1 = GradientBoostingClassifier()\n",
    "    gb_clf2 = GradientBoostingClassifier()\n",
    "    gb_clf3 = GradientBoostingClassifier()\n",
    "\n",
    "    gb_clf1.fit(x_init1,y_init1)\n",
    "    score_weight1 = gb_clf1.score(x_init,y_init)\n",
    "    \n",
    "    gb_clf2.fit(x_init2,y_init2)\n",
    "    score_weight2 = gb_clf2.score(x_init,y_init)\n",
    "    \n",
    "    gb_clf3.fit(x_init3,y_init3)\n",
    "    score_weight3 = gb_clf3.score(x_init,y_init)\n",
    "    \n",
    "    proba1 = gb_clf1.predict_proba(x_choice)\n",
    "    proba2 = gb_clf2.predict_proba(x_choice)\n",
    "    proba3 = gb_clf3.predict_proba(x_choice)\n",
    "    \n",
    "    scores1_sort = scores_func(proba1)  \n",
    "    scores2_sort = scores_func(proba2)\n",
    "    scores3_sort = scores_func(proba3)\n",
    "    \n",
    "    distribution_normal = y_init.value_counts()[1]\n",
    "    distribution_suspicious = y_init.value_counts()[2]\n",
    "    distribution = distribution_suspicious/(distribution_normal+distribution_suspicious)\n",
    "    \n",
    "    neigh = NearestNeighbors()\n",
    "    neigh.fit(x_all)\n",
    "    distance_number = neigh.kneighbors([x_choice.iloc[i] for i in range(len(x_choice))], density_scope, return_distance=False)\n",
    "    \n",
    "    scores_sort = []\n",
    "    if distribution >= bal_scale:\n",
    "        for i in range(len(scores1_sort)):\n",
    "            density = (10-pairwise_distances([x_choice.iloc[i]],x_all.iloc[distance_number[i][1:]],metric=\"cosine\").sum())/10\n",
    "            col = max(scores1_sort[i]*score_weight1,scores2_sort[i]*score_weight2,scores3_sort[i]*score_weight3)*density\n",
    "            scores_sort.append(col)\n",
    "    else:\n",
    "        for i in range(len(scores1_sort)):\n",
    "            density = (10-pairwise_distances([x_choice.iloc[i]],x_all.iloc[distance_number[i][1:]],metric=\"cosine\").sum())/10\n",
    "            if (proba1[i][0]+proba2[i][0]+proba3[i][0])/3 < 0.5:\n",
    "                col = max(scores1_sort[i]*score_weight1,scores2_sort[i]*score_weight2,scores3_sort[i]*score_weight3)*density\n",
    "            else:\n",
    "                col = max(scores1_sort[i]*score_weight1,scores2_sort[i]*score_weight2,scores3_sort[i]*score_weight3)*density*bigger_par\n",
    "            scores_sort.append(col)\n",
    "\n",
    "    x_init,y_init,x_choice,y_choice = new_dataset(x_init,y_init,x_choice,y_choice,scores_sort)\n",
    "    \n",
    "    return x_init,y_init,x_choice,y_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#多样性+密度+平衡+委员会加权 qbc\n",
    "def choice_diversity_density_balance_committee_weighting(x_init,y_init,x_choice,y_choice,bal_scale=0.35,bigger_par=0.6,density_scope=11):\n",
    "\n",
    "    choice_list = [i for i in range(len(x_init))]\n",
    "    random.shuffle(choice_list)\n",
    "    \n",
    "    num1 = int(len(x_init)/3)\n",
    "    num2 = int(len(x_init)*2/3)\n",
    "    \n",
    "    x_init1 = pd.concat([x_init.iloc[choice_list[:num1]]])\n",
    "    x_init2 = pd.concat([x_init.iloc[choice_list[num1:num2]]])\n",
    "    x_init3 = pd.concat([x_init.iloc[choice_list[num2:]]])\n",
    "    \n",
    "    y_init1 = pd.concat([y_init.iloc[choice_list[:num1]]])\n",
    "    y_init2 = pd.concat([y_init.iloc[choice_list[num1:num2]]])\n",
    "    y_init3 = pd.concat([y_init.iloc[choice_list[num2:]]])\n",
    "    \n",
    "    gb_clf1 = GradientBoostingClassifier()\n",
    "    gb_clf2 = GradientBoostingClassifier()\n",
    "    gb_clf3 = GradientBoostingClassifier()\n",
    "\n",
    "    gb_clf1.fit(x_init1,y_init1)\n",
    "    score_weight1 = gb_clf1.score(x_init,y_init)\n",
    "    \n",
    "    gb_clf2.fit(x_init2,y_init2)\n",
    "    score_weight2 = gb_clf2.score(x_init,y_init)\n",
    "    \n",
    "    gb_clf3.fit(x_init3,y_init3)\n",
    "    score_weight3 = gb_clf3.score(x_init,y_init)\n",
    "    \n",
    "    proba1 = gb_clf1.predict_proba(x_choice)\n",
    "    proba2 = gb_clf2.predict_proba(x_choice)\n",
    "    proba3 = gb_clf3.predict_proba(x_choice)\n",
    "    \n",
    "    scores1_sort = scores_func(proba1)  \n",
    "    scores2_sort = scores_func(proba2)\n",
    "    scores3_sort = scores_func(proba3)\n",
    "    \n",
    "    #关键\n",
    "    distribution_normal = y_init.value_counts()[1]\n",
    "    distribution_suspicious = y_init.value_counts()[2]\n",
    "    distribution = distribution_suspicious/(distribution_normal+distribution_suspicious)\n",
    "    \n",
    "    neigh = NearestNeighbors()\n",
    "    neigh.fit(x_all)\n",
    "    distance_number = neigh.kneighbors([x_choice.iloc[i] for i in range(len(x_choice))], density_scope, return_distance=False)\n",
    "    \n",
    "    scores_sort = []\n",
    "    if distribution >= bal_scale:\n",
    "        for i in range(len(scores1_sort)):\n",
    "            diversity = pairwise_distances([x_choice.iloc[i]],x_all.iloc[distance_number[i][1:2]],metric=\"cosine\").sum()\n",
    "            density = (10-pairwise_distances([x_choice.iloc[i]],x_all.iloc[distance_number[i][1:]],metric=\"cosine\").sum())/10\n",
    "            col = max(scores1_sort[i]*score_weight1,scores2_sort[i]*score_weight2,scores3_sort[i]*score_weight3)*diversity*density\n",
    "            scores_sort.append(col)\n",
    "    else:\n",
    "        for i in range(len(scores1_sort)):\n",
    "            diversity = pairwise_distances([x_choice.iloc[i]],x_all.iloc[distance_number[i][1:2]],metric=\"cosine\").sum()\n",
    "            density = (10-pairwise_distances([x_choice.iloc[i]],x_all.iloc[distance_number[i][1:]],metric=\"cosine\").sum())/10\n",
    "            if (proba1[i][0]+proba2[i][0]+proba3[i][0])/3 < 0.5:\n",
    "                col = max(scores1_sort[i]*score_weight1,scores2_sort[i]*score_weight2,scores3_sort[i]*score_weight3)*diversity*density\n",
    "            else:\n",
    "                col = max(scores1_sort[i]*score_weight1,scores2_sort[i]*score_weight2,scores3_sort[i]*score_weight3)*diversity*density*bigger_par\n",
    "            scores_sort.append(col)\n",
    "\n",
    "    x_init,y_init,x_choice,y_choice = new_dataset(x_init,y_init,x_choice,y_choice,scores_sort)\n",
    "    \n",
    "    return x_init,y_init,x_choice,y_choice"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
